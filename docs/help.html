<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLSC: Technical Documentation</title>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            color: #333;
            max-width: 900px;
            margin: auto;
            background-color: #f9f9f9;
        }
        h1, h2, h3 {
            color: #2c3e50;
            border-bottom: 2px solid #eaeaea;
            padding-bottom: 10px;
        }
        h1 { font-size: 2.5em; border-bottom: none; }
        code {
            background-color: #f4f4f4;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Consolas', 'Monaco', monospace;
            color: #d63384;
        }
        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        pre code {
            background-color: transparent;
            color: inherit;
            padding: 0;
        }
        .container {
            background-color: #fff;
            padding: 40px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            border-radius: 8px;
        }
        .reference {
            padding-left: 20px;
            border-left: 4px solid #3498db;
            background-color: #f0f7fb;
            padding: 10px 10px 10px 20px;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Technical Documentation: CNNs for Geometric Shape Classification</h1>
        
        <h2>Abstract</h2>
        <p>This document provides a comprehensive theoretical and technical overview of the machine learning pipeline implemented in the <strong>MLSC</strong> (Machine Learning Square vs Circle) project. We explore the architectural decisions underpinning the <code>SimpleCNN</code> model, the mathematical foundations of the Backpropagation algorithm used for optimization, and the stochastic properties of the data generation process. This documentation is intended for an academic audience.</p>

        <h2>1. Introduction</h2>
        <p>The classification of geometric shapes from rasterized images is a fundamental problem in computer vision. This project implements a supervised learning framework using a Convolutional Neural Network (CNN) to distinguish between two classes: \(C_0\) (Circle) and \(C_1\) (Square).</p>

        <h2>2. Data Generation Methodology</h2>
        <p>The dataset \(\mathcal{D}\) is fundamentally synthetic, generated via a stochastic process defined in <code>mlsc/generate_data.py</code>.</p>

        <h3>2.1 Stochastic Sampling</h3>
        <p>Let \(I \in \mathbb{R}^{H \times W}\) be a grayscale image where \(H=W=64\). For a class \(y \in \{0, 1\}\), a sample \(x\) is generated by rendering a geometric primitive \(\phi_y\) with parameters \(\theta \sim P(\Theta)\).</p>
        
        <ul>
            <li><strong>Squares (\(y=1\)):</strong> The side length \(s \sim \mathcal{U}(10, 40)\). Top-left coordinate \((u, v)\) is sampled uniformly within valid bounds.</li>
            <li><strong>Circles (\(y=0\)):</strong> The radius \(r \sim \mathcal{U}(5, 20)\). Center \((c_x, c_y)\) is sampled uniformly within valid bounds.</li>
        </ul>

        <h2>3. Neural Network Architecture</h2>
        <p>The model \(\mathcal{M}\) is a Deep Convolutional Neural Network parameterized by weights \(W\).</p>

        <h3>3.1 Convolutional Layers</h3>
        <p>The core operation is the 2D cross-correlation:</p>
        <p>$$ (I * K)_{ij} = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} I(i+m, j+n) K(m, n) $$</p>
        <p>Where \(K\) is the learnable kernel. The architecture employs two blocks of the form \( f_l(x) = \text{MaxPool}(\sigma(\text{Conv}(x))) \), where \(\sigma(\cdot)\) is the ReLU activation function.</p>

        <h3>3.2 Dimensionality Reduction</h3>
        <p>Spatial downsampling is achieved via Max Pooling with a \(2 \times 2\) window and stride 2, reducing the feature map dimensions sequentially.</p>

        <h2>4. Optimization and Training</h2>
        
        <h3>4.1 Loss Function</h3>
        <p>Minimization via the Cross-Entropy Loss function:</p>
        <p>$$ \mathcal{L}(y_i, p_i) = - \sum_{c=0}^{C-1} y_{i,c} \log(p_{i,c}) $$</p>

        <h3>4.2 Backpropagation</h3>
        <p>Gradients are computed via the Backpropagation algorithm. For a weight \(w_{ij}^l\) in layer \(l\), the gradient is computed using the Chain Rule:</p>
        <p>$$ \frac{\partial L}{\partial w_{ij}^l} = \frac{\partial L}{\partial a_j^l} \cdot \frac{\partial a_j^l}{\partial z_j^l} \cdot \frac{\partial z_j^l}{\partial w_{ij}^l} $$</p>

        <h2>5. Usage</h2>
        <p>To replicate the experiments:</p>
        <pre><code>uv sync
uv run mlsc generate
uv run mlsc train</code></pre>

        <h2>6. References</h2>
        <div class="reference">
            <strong>LeCun, Y., et al. (1998).</strong> "Gradient-based learning applied to document recognition." <em>Proceedings of the IEEE</em>.
        </div>
        <div class="reference">
            <strong>Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).</strong> "Learning representations by back-propagating errors." <em>Nature</em>.
        </div>
        <div class="reference">
            <strong>Kingma, D. P., & Ba, J. (2014).</strong> "Adam: A Method for Stochastic Optimization." <em>arXiv</em>.
        </div>
    </div>
</body>
</html>
