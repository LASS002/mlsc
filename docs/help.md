# Technical Documentation: Convolutional Neural Networks for Geometric Shape Classification

## Abstract

This document provides a comprehensive theoretical and technical overview of the machine learning pipeline implemented in the **MLSC** (Machine Learning Square vs Circle) project. We explore the architectural decisions underpinning the `SimpleCNN` model, the mathematical foundations of the Backpropagation algorithm used for optimization, and the stochastic properties of the data generation process. This documentation is intended for an academic audience and assumes familiarity with linear algebra and calculus.

## 1. Introduction

The classification of geometric shapes from rasterized images is a fundamental problem in computer vision, serving as a proxy for more complex object recognition tasks. This project implements a supervised learning framework using a Convolutional Neural Network (CNN) to distinguish between two classes: $C_0$ (Circle) and $C_1$ (Square).

## 2. Data Generation Methodology

The dataset $\mathcal{D}$ is fundamentally synthetic, generated via a stochastic process defined in `mlsc/generate_data.py`.

### 2.1 Stochastic Sampling

Let $I \in \mathbb{R}^{H \times W}$ be a grayscale image where $H=W=64$.
For a class $y \in \{0, 1\}$, a sample $x$ is generated by rendering a geometric primitive $\phi_y$ with parameters $\theta \sim P(\Theta)$.

* **Squares ($y=1$):**
    The side length $s$ is sampled uniformly: $s \sim \mathcal{U}(10, 40)$.
    The top-left coordinate $(u, v)$ is sampled such that the shape remains within bounds:
    $u \sim \mathcal{U}(0, W-s), \quad v \sim \mathcal{U}(0, H-s)$.

* **Circles ($y=0$):**
    The radius $r$ is sampled uniformly: $r \sim \mathcal{U}(5, 20)$.
    The center $(c_x, c_y)$ is sampled:
    $c_x \sim \mathcal{U}(r, W-r), \quad c_y \sim \mathcal{U}(r, H-r)$.

This ensures positional and scale invariance is required for the model to generalize effectively.

## 3. Neural Network Architecture

The model $\mathcal{M}$ is a Deep Convolutional Neural Network parameterized by weights $W$.

### 3.1 Convolutional Layers

The core operation is the 2D cross-correlation (referred to as convolution in DL literature):

$$ (I * K)_{ij} = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} I(i+m, j+n) K(m, n) $$

Where $K$ is the learnable kernel.
The architecture employs two blocks of the form:
$$ f_l(x) = \text{MaxPool}(\sigma(\text{Conv}(x))) $$
where $\sigma(\cdot)$ is the Rectified Linear Unit (ReLU) activation function:
$$ \sigma(z) = \max(0, z) $$

### 3.2 Dimensionality Reduction

Spatial downsampling is achieved via Max Pooling with a $2 \times 2$ window and stride 2:
$$ y_{ij} = \max_{m,n \in \{0,1\}} x_{2i+m, 2j+n} $$
This reduces the feature map dimensions from $64 \times 64 \rightarrow 32 \times 32 \rightarrow 16 \times 16$, introducing local translation invariance.

## 4. Optimization and Training

### 4.1 Loss Function

The problem is formulated as a multiclass classification task minimized via the Cross-Entropy Loss function. For a single sample $i$ with true class $y_i$ and predicted probability vector $p_i$:

$$ \mathcal{L}(y_i, p_i) = - \sum_{c=0}^{C-1} y_{i,c} \log(p_{i,c}) $$

### 4.2 Backpropagation

 Gradients are computed via the Backpropagation algorithm (Rumelhart et al., 1986).
 Let $L$ be the loss. For a weight $w_{ij}^l$ in layer $l$, the gradient is computed using the Chain Rule:

$$ \frac{\partial L}{\partial w_{ij}^l} = \frac{\partial L}{\partial a_j^l} \cdot \frac{\partial a_j^l}{\partial z_j^l} \cdot \frac{\partial z_j^l}{\partial w_{ij}^l} $$

where $z$ is the weighted input and $a$ is the activation. This allows for the iterative update of weights via Stochastic Gradient Descent (SGD) or its variants like Adam (Kingma & Ba, 2014), which is used in this project.

## 5. Usage

To replicate the experiments:

1. **Environment Setup**:

    ```bash
    uv sync
    ```

2. **Data Synthesis**:

    ```bash
    uv run mlsc generate
    ```

3. **Model Training**:

    ```bash
    uv run mlsc train
    ```

## 6. References

1. **LeCun, Y., et al. (1998).** "Gradient-based learning applied to document recognition." *Proceedings of the IEEE*, 86(11), 2278-2324. (Foundational work on CNNs).
2. **Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).** "Learning representations by back-propagating errors." *Nature*, 323(6088), 533-536.
3. **Kingma, D. P., & Ba, J. (2014).** "Adam: A Method for Stochastic Optimization." *arXiv preprint arXiv:1412.6980*.
4. **Goodfellow, I., Bengio, Y., & Courville, A. (2016).** *Deep Learning*. MIT Press.

---
*Author: Antigravity AI*
*Date: 2026*
